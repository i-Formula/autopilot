{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demo\r",
    "In this notebook we'll use the model we trained to detect whether the robot is ``free``, ``turn left``, `` turn right`` or ``blocked`` to enable a collision avoidance behavior on the robot.  \r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torchvision\r\n",
    "\r\n",
    "model = torchvision.models.alexnet(pretrained=False)\r\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 4)\r\n",
    "model.load_state_dict(torch.load('i/best_model.pth'))\r\n",
    "device = torch.device('cuda')\r\n",
    "model = model.to(device)\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\r\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\r\n",
    "\r\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\r\n",
    "\r\n",
    "from jetbot import Robot\r\n",
    "\r\n",
    "robot = Robot()",
    "\r\n",
    "def preprocess(camera_value):\r\n",
    "    global device, normalize\r\n",
    "    x = camera_value\r\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\r\n",
    "    x = x.transpose((2, 0, 1))\r\n",
    "    x = torch.from_numpy(x).float()\r\n",
    "    x = normalize(x)\r\n",
    "    x = x.to(device)\r\n",
    "    x = x[None, ...]\r\n",
    "    return x\r\n",
    "\r\n",
    "import traitlets\r\n",
    "from IPython.display import display\r\n",
    "import ipywidgets.widgets as widgets\r\n",
    "from jetbot import Camera, bgr8_to_jpeg\r\n",
    "\r\n",
    "camera = Camera.instance(width=224, height=224)\r\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\r\n",
    "#blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\r\n",
    "#speed_slider = widgets.FloatSlider(description='speed', min=0.0, max=0.5, value=0.0, step=0.01, orientation='horizontal')\r\n",
    "\r\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\r\n",
    "\r\n",
    "#display(widgets.VBox(image blocked_slider]), speed_slider]))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\r\n",
    "import time\r\n",
    "\r\n",
    "def update(change):\r\n",
    "    global blocked_slider, robot\r\n",
    "    x = change['new'] \r\n",
    "    x = preprocess(x)\r\n",
    "    with torch.no_grad():\r\n",
    "        y = model(x)\r\n",
    "    \r\n",
    "    classes = ['blocked','free', 'left','right']\r\n",
    "    _, predicted = torch.max(y, 1)\r\n",
    "    percentage = torch.softmax(y, dim=1)[0]*100\r\n",
    "    \r\n",    
    "    print(f'{classes[predicted[0]]}, {percentage[predicted[0]]}.item()')\r\n",
    "    \r\n",
    "    if classes[predicted[0]]=='blocked':\r\n",
    "        robot.backward(0.4)\r\n",
    "        time.sleep(0.1)\r\n",
    "    elif classes[predicted[0]]=='left':\r\n",
    "        robot.left(0.4)\r\n",
    "        time.sleep(0.1)\r\n",
    "    elif classes[predicted[0]]=='right':\r\n",
    "        robot.right(0.4)\r\n",
    "        time.sleep(0.1)\r\n",
    "    elif classes[predicted[0]]=='forward':\r\n",
    "        robot.forward(0.4)\r\n",
    "        time.sleep(0.1)\r\n",
    "    else:\r\n",
    "        robot.stop()\r\n",
    "    \r\n",
    "    ",
    "    \r\n",
    "    time.sleep(0.001)\r\n",
    "        \r\n",
    "display(widgets.VBox([image]))\r\n",
    "update({'new': camera.value})  # we call the function once to initialize\r\n",   
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you finish\r\n",
    "You must unattach the callback and stop the camera by executing the code below before you finish all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "\r\n",
    "camera.unobserve(update, names='value')\r\n",
    "\r\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\r\n",
    "\r\n",
    "robot.stop()\r\n",
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "That's it for this live demo!  Hopefully you had some fun and your robot avoided collisions intelligently! \n",
    "\n",
    "If your robot wasn't avoiding collisions very well, try to spot where it fails.  The beauty is that we can collect more data for these failure scenarios\n",
    "and the robot should get even better :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
